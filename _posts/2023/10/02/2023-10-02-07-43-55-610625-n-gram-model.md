---
layout: post
title: "[python] N-gram model"
description: " "
date: 2023-10-02
tags: [python]
comments: true
share: true
---

Language modeling plays a crucial role in various natural language processing tasks, such as speech recognition, machine translation, and text generation. One of the popular approaches used for language modeling is the **N-gram Model**.

The N-gram model predicts the probability of the next word in a sequence of words based on a fixed number of preceding words. The "N" in N-gram represents the number of preceding words taken into consideration. For example, a bi-gram model (N=2) would consider the probability of the next word based on the immediately preceding word, while a tri-gram model (N=3) would consider the two preceding words.

## Why Use N-gram Model?

The N-gram model makes a fundamental assumption called the *Markov assumption*. It assumes that the probability of a word only depends on the preceding N-1 words and is independent of the remaining words in the sequence. Despite this simplification, the N-gram model has proven to be effective in capturing local word dependencies and producing reasonable text samples.

## Implementation of N-gram Model in Python

Let's take a look at a basic Python implementation of the N-gram model using a tri-gram approach.

```python
import random
from collections import defaultdict

class NgramModel:
    def __init__(self, n=3):
        self.n = n
        self.ngrams = defaultdict(list)
        self.start_words = []

    def train(self, corpus):
        # Preprocess corpus and split into sentences
        sentences = corpus.split('.')
        
        for sentence in sentences:
            words = sentence.split()
            if len(words) < self.n:
                continue

            # Collect start words
            self.start_words.append(tuple(words[:self.n-1]))

            # Construct n-grams
            for i in range(len(words) - self.n + 1):
                ngram = tuple(words[i:i+self.n-1])
                self.ngrams[ngram].append(words[i+self.n-1])

    def generate_sentence(self):
        sentence = random.choice(self.start_words)
        while True:
            if sentence[-1] in ['.', '?', '!']:
                break
            next_word = random.choice(self.ngrams[sentence[-1]])
            sentence += (next_word,)
        return ' '.join(sentence)

# Example usage
corpus = "I love to code. Coding is my passion."
model = NgramModel(n=3)
model.train(corpus)
generated_sentence = model.generate_sentence()
print(generated_sentence)
```

In the code above, we define the `NgramModel` class that takes the value of N as a parameter during initialization. The `train` method takes a corpus of text and constructs the N-grams based on the sentences in the corpus. The `generate_sentence` method generates a sentence by randomly selecting words based on the constructed N-grams.

## Conclusion

The N-gram model provides a simple yet powerful approach to language modeling. By considering the preceding N-1 words, the model can capture local word dependencies and generate coherent text. Although it has its limitations, the N-gram model serves as a foundation for more advanced language models used in various natural language processing tasks.
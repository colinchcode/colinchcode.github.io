---
layout: post
title: "[python] Generative Adversarial Networks (GANs) in Python"
description: " "
date: 2023-10-19
tags: [python]
comments: true
share: true
---

Generative Adversarial Networks (GANs) are a type of deep learning model that can learn to generate new data that is similar to the training data. GANs consist of two components: a generator network and a discriminator network. The generator network generates new data samples, while the discriminator network tries to distinguish between real and generated data.

In this tutorial, we will implement a simple GAN using Python and the TensorFlow library. We will train the GAN on the MNIST dataset, which consists of handwritten digits.

## Table of Contents

- [Setting up the Environment](#setting-up-the-environment)
- [Building the Generator Network](#building-the-generator-network)
- [Building the Discriminator Network](#building-the-discriminator-network)
- [Training the GAN](#training-the-gan)
- [Generating New Images](#generating-new-images)
- [Conclusion](#conclusion)

## Setting up the Environment

First, let's set up our Python environment. We need to install the following dependencies:

```python
pip install tensorflow
pip install matplotlib
```

Next, we import the required libraries:

```python
import tensorflow as tf
from tensorflow.keras import layers
import matplotlib.pyplot as plt
```

## Building the Generator Network

The generator network takes random noise as input and generates new data samples. It consists of one or more layers of neural networks. We will use a simple fully connected network for our generator.

```python
def build_generator():
    model = tf.keras.Sequential()
    model.add(layers.Dense(256, input_shape=(100,), activation='relu'))
    model.add(layers.BatchNormalization())
    model.add(layers.Dense(512, activation='relu'))
    model.add(layers.BatchNormalization())
    model.add(layers.Dense(784, activation='tanh'))
    model.add(layers.Reshape((28, 28, 1)))
    return model

# Create the generator
generator = build_generator()

# Generate a random noise sample
noise = tf.random.normal([1, 100])
generated_image = generator(noise)

# Display the generated image
plt.imshow(generated_image[0, :, :, 0], cmap='gray')
plt.axis('off')
plt.show()
```

## Building the Discriminator Network

The discriminator network takes an input image and tries to classify whether it is real or generated by the generator network. It consists of one or more layers of neural networks. We will use a simple convolutional network for our discriminator.

```python
def build_discriminator():
    model = tf.keras.Sequential()
    model.add(layers.Conv2D(64, (3, 3), strides=(2, 2), padding='same', input_shape=(28, 28, 1)))
    model.add(layers.LeakyReLU())
    model.add(layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same'))
    model.add(layers.LeakyReLU())
    model.add(layers.Flatten())
    model.add(layers.Dense(1))
    return model

# Create the discriminator
discriminator = build_discriminator()

# Classify the generated image
prediction = discriminator(generated_image)

# Display the classification result (0 for generated image)
print(prediction)
```

## Training the GAN

Now, let's train the GAN using the generator and discriminator networks.

```python
# Loss function for the discriminator
cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)

# Loss function for the generator
def generator_loss(fake_output):
    return cross_entropy(tf.ones_like(fake_output), fake_output)

# Loss function for the discriminator
def discriminator_loss(real_output, fake_output):
    real_loss = cross_entropy(tf.ones_like(real_output), real_output)
    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)
    total_loss = real_loss + fake_loss
    return total_loss

# Optimizers for both networks
generator_optimizer = tf.keras.optimizers.Adam(1e-4)
discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)

# Training loop
@tf.function
def train_step(images):
    noise = tf.random.normal([BATCH_SIZE, 100])

    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
        generated_images = generator(noise, training=True)

        real_output = discriminator(images, training=True)
        fake_output = discriminator(generated_images, training=True)

        gen_loss = generator_loss(fake_output)
        disc_loss = discriminator_loss(real_output, fake_output)

    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)
    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)

    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))
    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))

# Train the GAN on the MNIST dataset
for epoch in range(EPOCHS):
    for image_batch in train_dataset:
        train_step(image_batch)

    # Display generated images every 10 epochs
    if (epoch + 1) % 10 == 0:
        generate_and_save_images(generator, epoch + 1, seed)

```

## Generating New Images

Finally, we can generate new images using the trained generator network.

```python
def generate_and_save_images(model, epoch, test_input):
    predictions = model(test_input, training=False)
    fig = plt.figure(figsize=(4, 4))

    for i in range(predictions.shape[0]):
        plt.subplot(4, 4, i + 1)
        plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')
        plt.axis('off')

    plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))
    plt.show()

# Generate new images using the trained generator
generate_and_save_images(generator, EPOCHS, seed)
```

## Conclusion

In this tutorial, we implemented a simple GAN using Python and TensorFlow. We built the generator and discriminator networks, trained the GAN on the MNIST dataset, and generated new images using the trained generator. GANs have many potential applications, such as generating realistic images, creating new music, and even generating text.